{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUK Coding Challenge\n",
    "\n",
    "Die Aufgabe besteht in der Modellierung einer Kundenaffinität zum Abschluss einer KFZ-Versicherung."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "from functools import reduce\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_delimiter(filename):\n",
    "    \"\"\"\n",
    "    This function looks for the delimiter in a file.\n",
    "\n",
    "    Inputs:\n",
    "        - filename (str): path to specific file\n",
    "    Returns:\n",
    "        - delimiter (str)\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', newline='') as file:\n",
    "        dialect = csv.Sniffer().sniff(file.read(1024))\n",
    "        return dialect.delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 12:02:29,556 Found files: ['rest.csv', 'interesse.csv', 'alter_geschlecht.csv']\n"
     ]
    }
   ],
   "source": [
    "input_folder_path = 'data/input_data/'\n",
    "directories = [os.path.join(os.getcwd(), '..', input_folder_path)]\n",
    "file_list = []\n",
    "dataframes = []\n",
    "\n",
    "# search in all specified directories\n",
    "for directory in directories:\n",
    "    # list content of directory\n",
    "    file_names = os.listdir(os.path.join(os.getcwd(), '..', directory))\n",
    "    logger.info(f'Found files: {file_names}')\n",
    "    for each_file_name in file_names:\n",
    "        file_list.append(each_file_name)\n",
    "        # get filepath to relevant files\n",
    "        file_path = os.path.join(os.getcwd(), '..', directory, each_file_name)\n",
    "        # error handling for the one file using a different delimiter\n",
    "        delimiter = detect_delimiter(file_path)\n",
    "        if delimiter == ',':\n",
    "            current_df = pd.read_csv(file_path)\n",
    "            dataframes.append(current_df)\n",
    "        else:\n",
    "            current_df = pd.read_csv(file_path, delimiter=';')\n",
    "            dataframes.append(current_df)\n",
    "\n",
    "# Merge all dataframes into one\n",
    "df_merged = reduce(lambda left, right: pd.merge(left, right, on='id'), dataframes)\n",
    "\n",
    "# Deduplicate\n",
    "df_merged = df_merged.drop_duplicates()\n",
    "\n",
    "# Check wether data path exists\n",
    "if not os.path.exists('../data/raw_data/'):\n",
    "    os.makedirs('../data/raw_data/')\n",
    "\n",
    "# save merged dataframe as csv\n",
    "df_merged.to_csv('../data/raw_data/raw_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Datenanalyse (EDA)\n",
    "\n",
    "Machen Sie sich mit dem Datensatz vertraut. Identifizieren Sie dabei mögliche Probleme sowie grundlegende statistische Zusammenhänge, welche für die anschließende Modellierung wichtig sein könnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c82f8675774b068ee9bf95c0001e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philipherp/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/multimethod/__init__.py:184: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  return self[tuple(map(self.get_type, args))](*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df22da110d145d495b3f305293d6a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c380d929d2f84ac9b2c18c0736694390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5161a5347e91488096f7e9efc3e33342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profile = ProfileReport(df_raw, title='Pandas Profiling Report on raw data')\n",
    "# open report from output.html file generated from this cell\n",
    "profile.to_file(\"../eda_output.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "\n",
    "- Number of variables:      12\n",
    "- Number of observations:   381109\n",
    "- Missing cells:            0\n",
    "- Missing cells %:          0%\n",
    "- Duplicate rows:           0\n",
    "- Categorical:              2\n",
    "- Numeric:                  8\n",
    "- Boolean:                  1\n",
    "- Variables:\n",
    "    - Fahrerlaubnis:                Highly imbalanced (97.8%)\n",
    "    - Vertriebskanal > Alter:       High correlation\n",
    "    - Vorversicherung > Vorschaden: High correlation\n",
    "    - Alter_Fzg > Vertriebskanal:   High correlation\n",
    "    - id:                           uniformly distributed & unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 381109 entries, 0 to 381108\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   Fahrerlaubnis    381109 non-null  int64  \n",
      " 1   Regional_Code    381109 non-null  float64\n",
      " 2   Vorversicherung  381109 non-null  int64  \n",
      " 3   Alter_Fzg        381109 non-null  object \n",
      " 4   Vorschaden       381109 non-null  object \n",
      " 5   Jahresbeitrag    381109 non-null  float64\n",
      " 6   Vertriebskanal   381109 non-null  float64\n",
      " 7   Kundentreue      381109 non-null  int64  \n",
      " 8   id               381109 non-null  int64  \n",
      " 9   Interesse        381109 non-null  float64\n",
      " 10  Geschlecht       381109 non-null  object \n",
      " 11  Alter            381109 non-null  int64  \n",
      "dtypes: float64(4), int64(5), object(3)\n",
      "memory usage: 45.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether data path exists\n",
    "if not os.path.exists('../data/cleaned_data/'):\n",
    "    os.makedirs('../data/cleaned_data/')\n",
    "\n",
    "# save merged dataframe as csv\n",
    "df_raw.to_csv('../data/cleaned_data/cleaned_data.csv')\n",
    "df_cleaned = df_raw.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Bereiten Sie, soweit für Ihre Modellierung nötig, die Variablen geeignet auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names of numerical columns\n",
    "column_names = df_cleaned.columns\n",
    "\n",
    "# Get categorical columns\n",
    "categorical_columns = df_cleaned.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Get numerical columns\n",
    "numerical_columns = df_cleaned.select_dtypes(include=\"number\").columns\n",
    "\n",
    "# Create pipeline for categorical columns\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create pipeline for numerical columns except Interesse\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create transformer for all columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical_pipeline', categorical_pipeline, categorical_columns),\n",
    "    ('numerical_pipeline', numerical_pipeline, numerical_columns)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Fit and transform data\n",
    "df_cleaned = preprocessor.fit_transform(df_cleaned)\n",
    "\n",
    "# Convert to dataframe\n",
    "df_cleaned = pd.DataFrame(df_cleaned, columns=['lag_1', \n",
    "                                               'lag_2', \n",
    "                                               'lag_3', \n",
    "                                               'lag_4', \n",
    "                                               'lag_5', \n",
    "                                               'lag_6', \n",
    "                                               'lag_7', \n",
    "                                               'Fahrerlaubnis', \n",
    "                                               'Regional_Code', \n",
    "                                               'Vorversicherung', \n",
    "                                               'Jahresbeitrag', \n",
    "                                               'Vertriebskanal', \n",
    "                                               'Kundentreue', \n",
    "                                               'id', \n",
    "                                               'Interesse', \n",
    "                                               'Alter'])\n",
    "\n",
    "# Put id column as first column\n",
    "# and Interesse as last column\n",
    "df_cleaned = df_cleaned[['id', \n",
    "                         'lag_1', \n",
    "                         'lag_2', \n",
    "                         'lag_3', \n",
    "                         'lag_4', \n",
    "                         'lag_5', \n",
    "                         'lag_6', \n",
    "                         'lag_7', \n",
    "                         'Fahrerlaubnis', \n",
    "                         'Regional_Code', \n",
    "                         'Vorversicherung', \n",
    "                         'Jahresbeitrag', \n",
    "                         'Vertriebskanal', \n",
    "                         'Kundentreue', \n",
    "                         'Alter', \n",
    "                         'Interesse']]                                             \n",
    "\n",
    "# change unnecessary floats to int\n",
    "float_columns = ['Fahrerlaubnis', \n",
    "                 'Regional_Code', \n",
    "                 'Vorversicherung', \n",
    "                 'Vertriebskanal', \n",
    "                 'Kundentreue', \n",
    "                 'Alter', \n",
    "                 'Interesse']\n",
    "df_cleaned[float_columns] = df_cleaned[float_columns].astype('int64')\n",
    "\n",
    "# Check wether data path exists\n",
    "if not os.path.exists('../data/encoded_data/'):\n",
    "    os.makedirs('../data/encoded_data/')\n",
    "\n",
    "# save merged dataframe as csv\n",
    "df_cleaned.to_csv('../data/encoded_data/encoded_data.csv')\n",
    "df_encoded = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>lag_6</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>Fahrerlaubnis</th>\n",
       "      <th>Regional_Code</th>\n",
       "      <th>Vorversicherung</th>\n",
       "      <th>Jahresbeitrag</th>\n",
       "      <th>Vertriebskanal</th>\n",
       "      <th>Kundentreue</th>\n",
       "      <th>Alter</th>\n",
       "      <th>Interesse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.155097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.622853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.340142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.622853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.274776</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.195222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.444796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083344</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.049795</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.133758</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  lag_1  lag_2  lag_3  lag_4  lag_5  lag_6  lag_7  Fahrerlaubnis  \\\n",
       "0  1.155097    1.0    0.0    0.0    1.0    0.0    0.0    1.0              0   \n",
       "1  1.340142    1.0    0.0    0.0    0.0    1.0    0.0    1.0              0   \n",
       "2 -0.274776    1.0    0.0    0.0    0.0    1.0    0.0    1.0              0   \n",
       "3 -0.444796    0.0    1.0    0.0    1.0    0.0    0.0    1.0              0   \n",
       "4 -1.049795    1.0    0.0    0.0    0.0    1.0    0.0    1.0              0   \n",
       "\n",
       "   Regional_Code  Vorversicherung  Jahresbeitrag  Vertriebskanal  Kundentreue  \\\n",
       "0              0                1      -1.622853               0            0   \n",
       "1              0                0      -1.622853               0            0   \n",
       "2              0                0      -0.195222               0            0   \n",
       "3              1                1       0.083344               0            1   \n",
       "4              1                0      -0.133758              -1           -1   \n",
       "\n",
       "   Alter  Interesse  \n",
       "0      2          0  \n",
       "1      0          0  \n",
       "2     -1          0  \n",
       "3      0          0  \n",
       "4      0          0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>Regional_Code</th>\n",
       "      <th>Vorversicherung</th>\n",
       "      <th>Jahresbeitrag</th>\n",
       "      <th>Vertriebskanal</th>\n",
       "      <th>Kundentreue</th>\n",
       "      <th>Alter</th>\n",
       "      <th>Interesse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.155097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.622853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.340142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.622853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.274776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.195222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.444796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083344</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.049795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.133758</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  lag_4  lag_5  Regional_Code  Vorversicherung  Jahresbeitrag  \\\n",
       "0  1.155097    1.0    0.0              0                1      -1.622853   \n",
       "1  1.340142    0.0    1.0              0                0      -1.622853   \n",
       "2 -0.274776    0.0    1.0              0                0      -0.195222   \n",
       "3 -0.444796    1.0    0.0              1                1       0.083344   \n",
       "4 -1.049795    0.0    1.0              1                0      -0.133758   \n",
       "\n",
       "   Vertriebskanal  Kundentreue  Alter  Interesse  \n",
       "0               0            0      2          0  \n",
       "1               0            0      0          0  \n",
       "2               0            0     -1          0  \n",
       "3               0            1      0          0  \n",
       "4              -1           -1      0          0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop features from df_encode with feature importance below 0.05\n",
    "df_encoded = df_encoded.drop(['lag_1',\n",
    "                              'lag_2',\n",
    "                              'lag_3',\n",
    "                              'lag_6',\n",
    "                              'lag_7',\n",
    "                              'Fahrerlaubnis' ], axis=1)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.Interesse.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellvergleich\n",
    "\n",
    "Entscheiden Sie sich für ein geeignetes Modell zur Prognose der Kundenaffinität. Erläutern Sie wie Sie dabei vorgehen und begründen Sie Ihre Entscheidung."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Kundenaffinität zum Abschluss einer Kfz-Versicherung vorherzusagen, empfehle ich die Verwendung eines binären Klassifikationsmodells, wie beispielsweise das logistische Regressionsmodell oder ein Random Forest Classifier. Beide Modelle eignen sich gut für diese Art von Prognoseaufgaben.\n",
    "\n",
    "Logistische Regression:\n",
    "Die logistische Regression ist eine weit verbreitete Methode zur Vorhersage von binären Ergebnissen. Sie modelliert die Wahrscheinlichkeit, dass eine Beobachtung einer bestimmten Klasse angehört, basierend auf einer Kombination von Eingangsvariablen. In diesem Fall könnten Merkmale wie Alter, Geschlecht, Fahrzeugtyp, Vorversicherungshistorie, Schadensfreiheitsklasse, geografischer Standort usw. als Eingangsvariablen dienen. Das Modell kann dann die Wahrscheinlichkeit schätzen, dass ein Kunde affin oder nicht affin ist und eine Entscheidungsgrenze festlegen, um die Vorhersage zu treffen.\n",
    "\n",
    "Random Forest Classifier:\n",
    "Ein Random Forest Classifier ist ein Ensemble-Modell, das aus mehreren Entscheidungsbäumen besteht. Jeder Baum wird auf einem zufälligen Teil des Datensatzes trainiert, und die Vorhersage erfolgt durch Abstimmung der Vorhersagen der einzelnen Bäume. Random Forests sind in der Regel robust gegenüber Overfitting und können gut mit einer Mischung aus kategorischen und numerischen Variablen umgehen. Sie können auch die wichtigsten Merkmale identifizieren, die zur Vorhersage beitragen.\n",
    "\n",
    "Bei der Auswahl des Modells sind folgende Faktoren zu berücksichtigen:\n",
    "\n",
    "Datenverfügbarkeit: Verfügbarkeit der Daten überprüfen. Sicherstellen, dass ausreichend Daten vorhanden sind, um ein zuverlässiges Modell zu trainieren, und dass die relevanten Merkmale erfasst werden.\n",
    "\n",
    "Interpretierbarkeit: Wenn es wichtig ist, die Vorhersage des Modells zu verstehen und zu erklären, könnte die logistische Regression die bessere Wahl sein. Die Koeffizienten des Modells können direkt interpretiert werden, um den Einfluss der einzelnen Merkmale zu verstehen.\n",
    "\n",
    "Leistung: Gründliche Evaluation der Modelle durchführen, indem geeignete Leistungsmetriken wie Genauigkeit, Präzision, Recall oder den Flächenwert unter der ROC-Kurve (AUC-ROC) verwendet werden. Geeignetes Modell mit den besten Vorhersageergebnissen für Ihre spezifische Anwendung auswählen.\n",
    "\n",
    "Dateninterpretation: Wenn interessant ist, zu verstehen, welche Merkmale am stärksten zur Vorhersage beitragen, könnte der Random Forest Classifier von Vorteil sein. Er kann die wichtigsten Merkmale identifizieren und Einblicke in die Beziehung zwischen den Merkmalen und der Zielvariable liefern.\n",
    "\n",
    "Es ist grundsätzlich sinnvoll im Prozess verschiedene Modelle miteinander zu vergleichen, diese Code Challenge wird allerdings mit einem Random Forrest Classifier umgesetzt werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellbuilding\n",
    "\n",
    "1. Trainieren Sie das von Ihnen gewählte Modell. Wählen Sie geeignete Metriken um die Güte des finalen Modells zu beurteilen.\n",
    "2. Zeigen Sie, welche Variablen und Zusammenhänge für Ihre finales Modell relevant sind.\n",
    "3. Überlegen Sie sich (ohne Umsetzung) wie Sie Ihr Modell weiter optimieren können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random oversampler to balance dataset on Interesse = 1\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(df_encoded.drop(['Interesse'], axis=1), df_encoded['Interesse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 12:03:59,013 Train Accuracy: 0.9999786396900192\n",
      "2023-05-20 12:03:59,115 Train Precision: 0.9999615251433188\n",
      "2023-05-20 12:03:59,221 Train Recall: 0.9999957248697154\n",
      "2023-05-20 12:03:59,324 Train F1: 0.9999786247141055\n"
     ]
    }
   ],
   "source": [
    "# Train a random forest classifier model \n",
    "# and select suitable hyperparameters\n",
    "# and metrics to view the models performance\n",
    "\n",
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, \n",
    "                                                    y_resampled, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns=['Interesse'])\n",
    "\n",
    "# Predict on train set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Evaluate model on train set\n",
    "logger.info(f'Train Accuracy: {accuracy_score(y_train, y_train_pred)}')\n",
    "logger.info(f'Train Precision: {precision_score(y_train, y_train_pred, pos_label=2)}')\n",
    "logger.info(f'Train Recall: {recall_score(y_train, y_train_pred, pos_label=2)}')\n",
    "logger.info(f'Train F1: {f1_score(y_train, y_train_pred, pos_label=2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the models performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 12:04:03,537 Tets Accuracy: 0.9298096092503987\n",
      "2023-05-20 12:04:03,582 Test Precision: 0.8794652660055687\n",
      "2023-05-20 12:04:03,628 Test Recall: 0.9964174826844996\n",
      "2023-05-20 12:04:03,673 Test F1: 0.9342956718096099\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "logger.info(f'Tets Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "logger.info(f'Test Precision: {precision_score(y_test, y_pred, pos_label=2)}')\n",
    "logger.info(f'Test Recall: {recall_score(y_test, y_pred, pos_label=2)}')\n",
    "logger.info(f'Test F1: {f1_score(y_test, y_pred, pos_label=2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Score:\n",
    "Accuracy_score calculates the percentage of correct predictions made by the model out of all the predictions. It is calculated by dividing the number of correct predictions by the total number of predictions. A high accuracy score indicates a high overall predictive performance of the model. However, accuracy alone might not be sufficient if the dataset is imbalanced.\n",
    "\n",
    "Precision Score:\n",
    "Precision_score measures the proportion of correctly predicted positive instances out of all instances predicted as positive. It is calculated by dividing the number of true positives by the sum of true positives and false positives. Precision focuses on the quality of positive predictions. A high precision score indicates a low false positive rate, meaning that when the model predicts a positive class, it is likely to be correct.\n",
    "\n",
    "Recall Score:\n",
    "Recall_score, also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances out of all actual positive instances. It is calculated by dividing the number of true positives by the sum of true positives and false negatives. Recall focuses on the model's ability to find all positive instances without missing any. A high recall score indicates a low false negative rate, meaning that the model can correctly identify a large proportion of positive instances.\n",
    "\n",
    "F1 Score:\n",
    "The F1_score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is useful when you want to consider both false positives and false negatives. It is calculated as 2 * ((precision * recall) / (precision + recall)). The F1 score is a single metric that combines precision and recall. A high F1 score indicates good overall performance when considering both precision and recall.\n",
    "\n",
    "In summary, while accuracy_score provides an overall performance measure, precision_score, recall_score, and f1_score offer insights into different aspects of the model's performance. Consider your specific requirements and the nature of your problem to determine which metric(s) are most important for your evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether data path exists\n",
    "if not os.path.exists('../models/'):\n",
    "    os.makedirs('../models/')\n",
    "\n",
    "# Save model with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "with open(f'../models/model_{timestamp}.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible strategies to optimize the Random Forrest Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Feature Selection:__\n",
    "\n",
    "Evaluate the importance of each feature in your model and consider removing less informative or highly correlated features. This can be done by examining the feature importances provided by the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.343205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jahresbeitrag</th>\n",
       "      <td>0.234458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_4</th>\n",
       "      <td>0.160646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_5</th>\n",
       "      <td>0.136745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vorversicherung</th>\n",
       "      <td>0.082752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alter</th>\n",
       "      <td>0.029289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vertriebskanal</th>\n",
       "      <td>0.004912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kundentreue</th>\n",
       "      <td>0.004499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regional_Code</th>\n",
       "      <td>0.003493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 importance\n",
       "id                 0.343205\n",
       "Jahresbeitrag      0.234458\n",
       "lag_4              0.160646\n",
       "lag_5              0.136745\n",
       "Vorversicherung    0.082752\n",
       "Alter              0.029289\n",
       "Vertriebskanal     0.004912\n",
       "Kundentreue        0.004499\n",
       "Regional_Code      0.003493"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the most important features\n",
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Hyperparameter Tuning:__ \n",
    "\n",
    "Random forests have various hyperparameters that can be tuned to improve performance and reduce overfitting. Some key hyperparameters include the number of trees (n_estimators), the maximum depth of the trees (max_depth), and the minimum number of samples required to split an internal node (min_samples_split). Using techniques like grid search or randomized search, you can find optimal hyperparameter values that balance model complexity and performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Regularization:__ \n",
    "\n",
    "Random forests have regularization techniques like subsampling (using a subset of samples for training each tree) and feature subspace sampling (using a subset of features for each split). These techniques can help reduce overfitting by introducing additional randomness into the model.\n",
    "\n",
    "__Subsampling (Bootstrap Aggregating or Bagging):__\n",
    "In a random forest, each decision tree is trained on a different bootstrap sample obtained by randomly selecting a subset of the original training data with replacement. This means that each tree is trained on a different subset of the data, allowing them to capture different patterns. By aggregating the predictions of all trees, the random forest reduces the variance and prevents overfitting. Subsampling introduces additional randomness into the model, improving its generalization.\n",
    "\n",
    "__Feature Subspace Sampling:__\n",
    "For each split in a decision tree within a random forest, only a random subset of features is considered. Instead of evaluating all features at each split, a limited set of features is randomly selected. This technique is also called feature bagging or random subspace method. By using only a subset of features for each tree, the random forest encourages diversity among the trees and reduces the correlation between them. This helps prevent overfitting and improves the robustness of the model.\n",
    "\n",
    "**Regularization techniques like subsampling and feature subspace sampling are inherent in the random forest algorithm and do not require explicit user intervention. However, you can adjust the hyperparameters max_features and max_samples to fine-tune the regularization strength and control the trade-off between model complexity and performance.**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Cross-Validation:__ \n",
    "Instead of evaluating the model solely on the training set, perform cross-validation to assess its performance on multiple train-test splits. This helps provide a more robust estimate of the model's performance and can indicate if it is overfitting.\n",
    "\n",
    "The advantage of using cross-validation is that it provides a more robust estimate of the model's performance compared to a single train-test split. It helps assess the model's ability to generalize to unseen data and reduces the influence of the specific data split on the evaluation.\n",
    "To perform cross-validation in scikit-learn, you can use the cross_val_score function or the cross_validate function, specifying the number of folds (cv parameter) and the desired evaluation metric. These functions handle the data splitting and model evaluation automatically, making it easier to perform cross-validation with random forests."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Increase Training Data:__ \n",
    "\n",
    "If possible, obtain more training data to provide a broader representation of the underlying patterns. A larger dataset can help the model generalize better and reduce overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Early Stopping:__ \n",
    "\n",
    "Monitor the model's performance on a validation set during training and stop training early if the performance starts to degrade. This prevents the model from overfitting to the training data excessively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5625 candidates, totalling 16875 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 102\u001b[0m\n\u001b[1;32m     98\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator \u001b[39m=\u001b[39m rf, param_grid \u001b[39m=\u001b[39m param_grid,\n\u001b[1;32m     99\u001b[0m                             cv \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, n_jobs \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[39m# Fit the grid search to the data\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m    104\u001b[0m \u001b[39m# Get best parameters\u001b[39;00m\n\u001b[1;32m    105\u001b[0m grid_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1389\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1388\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train a random forest classifier model\n",
    "# select suitable hyperparamters with grit search\n",
    "# select metrics to view the models performance\n",
    "\n",
    "# Get categorical columns\n",
    "categorical_columns = df_cleaned.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Get numerical columns\n",
    "numerical_columns = df_cleaned.select_dtypes(include=\"number\").columns\n",
    "\n",
    "# Create pipeline for categorical columns\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create pipeline for numerical columns except Interesse\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create transformer for all columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical_pipeline', categorical_pipeline, categorical_columns),\n",
    "    {'standard_scaler', numerical_pipeline, numerical_columns}\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Fit and transform data\n",
    "df_cleaned = preprocessor.fit_transform(df_cleaned)\n",
    "\n",
    "# Convert to dataframe\n",
    "df_cleaned = pd.DataFrame(df_cleaned, columns=['lag_1', \n",
    "                                               'lag_2', \n",
    "                                               'lag_3', \n",
    "                                               'lag_4', \n",
    "                                               'lag_5', \n",
    "                                               'lag_6', \n",
    "                                               'lag_7', \n",
    "                                               'Fahrerlaubnis', \n",
    "                                               'Regional_Code', \n",
    "                                               'Vorversicherung', \n",
    "                                               'Jahresbeitrag', \n",
    "                                               'Vertriebskanal', \n",
    "                                               'Kundentreue', \n",
    "                                               'id', \n",
    "                                               'Interesse', \n",
    "                                               'Alter'])\n",
    "\n",
    "# Put id column as first column\n",
    "# and Interesse as last column\n",
    "df_cleaned = df_cleaned[['id', \n",
    "                         'lag_1', \n",
    "                         'lag_2', \n",
    "                         'lag_3', \n",
    "                         'lag_4', \n",
    "                         'lag_5', \n",
    "                         'lag_6', \n",
    "                         'lag_7', \n",
    "                         'Fahrerlaubnis', \n",
    "                         'Regional_Code', \n",
    "                         'Vorversicherung', \n",
    "                         'Jahresbeitrag', \n",
    "                         'Vertriebskanal', \n",
    "                         'Kundentreue', \n",
    "                         'Alter', \n",
    "                         'Interesse']]\n",
    "\n",
    "# change unnecessary floats to int\n",
    "float_columns = ['Fahrerlaubnis', \n",
    "                 'Regional_Code', \n",
    "                 'Vorversicherung', \n",
    "                 'Vertriebskanal', \n",
    "                 'Kundentreue', \n",
    "                 'Alter']\n",
    "df_cleaned[float_columns] = df_cleaned[float_columns].astype('int64')\n",
    "\n",
    "df_encoded = df_cleaned.copy()\n",
    "\n",
    "# Train a random forest classifier model\n",
    "# select suitable hyperparamters with grit search\n",
    "# select metrics to view the models performance\n",
    "\n",
    "# train, test, split\n",
    "# Split data into train and test\n",
    "X = df_encoded.drop(['Interesse'], axis=1)\n",
    "y = df_encoded['Interesse']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_samples_leaf': [3, 4, 5, 6, 7],\n",
    "    'min_samples_split': [8, 10, 12, 14, 16],\n",
    "    'n_estimators': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,\n",
    "                            cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters\n",
    "grid_search.best_params_\n",
    "\n",
    "# Get best estimator\n",
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "# Get metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=2)\n",
    "recall = recall_score(y_test, y_pred, pos_label=2)\n",
    "f1 = f1_score(y_test, y_pred, pos_label=2)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1: {f1}')\n",
    "\n",
    "# Check wether data path exists\n",
    "if not os.path.exists('../models/'):\n",
    "    os.makedirs('../models/')\n",
    "\n",
    "# save model\n",
    "pickle.dump(best_grid, open('../models/model.pkl', 'wb'))\n",
    "\n",
    "# save preprocessor\n",
    "pickle.dump(preprocessor, open('../models/preprocessor.pkl', 'wb'))\n",
    "\n",
    "# save metrics\n",
    "metrics = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "pickle.dump(metrics, open('../models/metrics.pkl', 'wb'))\n",
    "\n",
    "# save predictions\n",
    "predictions = {'y_pred': y_pred}\n",
    "pickle.dump(predictions, open('../models/predictions.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huk_coding_challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "913db32498f814a7e956b858dc9ab26e0e62ace447c646841c6d05c67a28a2e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
