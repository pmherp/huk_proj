{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUK Coding Challenge\n",
    "\n",
    "Die Aufgabe besteht in der Modellierung einer Kundenaffinität zum Abschluss einer KFZ-Versicherung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "from functools import reduce\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_delimiter(filename):\n",
    "    \"\"\"\n",
    "    This function looks for the delimiter in a file.\n",
    "\n",
    "    Inputs:\n",
    "        - filename (str): path to specific file\n",
    "    Returns:\n",
    "        - delimiter (str)\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', newline='') as file:\n",
    "        dialect = csv.Sniffer().sniff(file.read(1024))\n",
    "        return dialect.delimiter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 17:26:10,020 Found files: ['rest.csv', 'interesse.csv', 'alter_geschlecht.csv']\n"
     ]
    }
   ],
   "source": [
    "input_folder_path = 'data/input_data/'\n",
    "directories = [os.path.join(os.getcwd(), '..', input_folder_path)]\n",
    "file_list = []\n",
    "dataframes = []\n",
    "\n",
    "# search in all specified directories\n",
    "for directory in directories:\n",
    "    # list content of directory\n",
    "    file_names = os.listdir(os.path.join(os.getcwd(), '..', directory))\n",
    "    logger.info(f'Found files: {file_names}')\n",
    "    for each_file_name in file_names:\n",
    "        file_list.append(each_file_name)\n",
    "        # get filepath to relevant files\n",
    "        file_path = os.path.join(os.getcwd(), '..', directory, each_file_name)\n",
    "        # error handling for the one file using a different delimiter\n",
    "        delimiter = detect_delimiter(file_path)\n",
    "        if delimiter == ',':\n",
    "            current_df = pd.read_csv(file_path)\n",
    "            dataframes.append(current_df)\n",
    "        else:\n",
    "            current_df = pd.read_csv(file_path, delimiter=';')\n",
    "            dataframes.append(current_df)\n",
    "\n",
    "# Merge all dataframes into one\n",
    "df_merged = reduce(lambda left, right: pd.merge(left, right, on='id'), dataframes)\n",
    "\n",
    "# Deduplicate\n",
    "df_merged = df_merged.drop_duplicates()\n",
    "\n",
    "# Check wether data path exists\n",
    "if not os.path.exists('../data/raw_data/'):\n",
    "    os.makedirs('../data/raw_data/')\n",
    "\n",
    "# save merged dataframe as csv\n",
    "df_merged.to_csv('../data/raw_data/raw_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Datenanalyse (EDA)\n",
    "\n",
    "Machen Sie sich mit dem Datensatz vertraut. Identifizieren Sie dabei mögliche Probleme sowie grundlegende statistische Zusammenhänge, welche für die anschließende Modellierung wichtig sein könnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5625830bcd84ae08a9aecf58bf6168b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philipherp/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/multimethod/__init__.py:184: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  return self[tuple(map(self.get_type, args))](*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4800f08955d647e1ab1824160073ceef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e0acbe303d43bf8ad9157789e15408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48cd29403a34622b015707f3bf595af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profile = ProfileReport(df_raw, title='Pandas Profiling Report on raw data')\n",
    "# open report from output.html file generated from this cell\n",
    "profile.to_file(\"../eda_output.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    334399\n",
       "1.0     46710\n",
       "Name: Interesse, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count values in column Interesse --> Imbalanced Dataset\n",
    "df_raw['Interesse'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fahrerlaubnis</th>\n",
       "      <th>Regional_Code</th>\n",
       "      <th>Vorversicherung</th>\n",
       "      <th>Alter_Fzg</th>\n",
       "      <th>Vorschaden</th>\n",
       "      <th>Jahresbeitrag</th>\n",
       "      <th>Vertriebskanal</th>\n",
       "      <th>Kundentreue</th>\n",
       "      <th>id</th>\n",
       "      <th>Interesse</th>\n",
       "      <th>Geschlecht</th>\n",
       "      <th>Alter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>74</td>\n",
       "      <td>317635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>213</td>\n",
       "      <td>337993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>27204.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>114</td>\n",
       "      <td>160325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>31999.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>251</td>\n",
       "      <td>141620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>28262.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>60</td>\n",
       "      <td>75060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fahrerlaubnis  Regional_Code  Vorversicherung Alter_Fzg Vorschaden  \\\n",
       "0              1           15.0                1  1-2 Year         No   \n",
       "1              1           28.0                0  1-2 Year        Yes   \n",
       "2              1           33.0                0  1-2 Year        Yes   \n",
       "3              1           46.0                1  < 1 Year         No   \n",
       "4              1           49.0                0  1-2 Year        Yes   \n",
       "\n",
       "   Jahresbeitrag  Vertriebskanal  Kundentreue      id  Interesse Geschlecht  \\\n",
       "0         2630.0           124.0           74  317635        0.0       Male   \n",
       "1         2630.0           125.0          213  337993        0.0       Male   \n",
       "2        27204.0           124.0          114  160325        0.0       Male   \n",
       "3        31999.0           152.0          251  141620        0.0       Male   \n",
       "4        28262.0            26.0           60   75060        0.0       Male   \n",
       "\n",
       "   Alter  \n",
       "0     76  \n",
       "1     43  \n",
       "2     20  \n",
       "3     24  \n",
       "4     51  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "\n",
    "- Number of variables:      12\n",
    "- Number of observations:   381109\n",
    "- Missing cells:            0\n",
    "- Missing cells %:          0%\n",
    "- Duplicate rows:           0\n",
    "- Categorical:              2\n",
    "- Numeric:                  8\n",
    "- Boolean:                  1\n",
    "- Variables:\n",
    "    - Fahrerlaubnis:                Highly imbalanced (97.8%)\n",
    "    - Vertriebskanal > Alter:       High correlation\n",
    "    - Vorversicherung > Vorschaden: High correlation\n",
    "    - Alter_Fzg > Vertriebskanal:   High correlation\n",
    "    - id:                           uniformly distributed & unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 381109 entries, 0 to 381108\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   Fahrerlaubnis    381109 non-null  int64  \n",
      " 1   Regional_Code    381109 non-null  float64\n",
      " 2   Vorversicherung  381109 non-null  int64  \n",
      " 3   Alter_Fzg        381109 non-null  object \n",
      " 4   Vorschaden       381109 non-null  object \n",
      " 5   Jahresbeitrag    381109 non-null  float64\n",
      " 6   Vertriebskanal   381109 non-null  float64\n",
      " 7   Kundentreue      381109 non-null  int64  \n",
      " 8   id               381109 non-null  int64  \n",
      " 9   Interesse        381109 non-null  float64\n",
      " 10  Geschlecht       381109 non-null  object \n",
      " 11  Alter            381109 non-null  int64  \n",
      "dtypes: float64(4), int64(5), object(3)\n",
      "memory usage: 45.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether data path exists\n",
    "if not os.path.exists('../data/cleaned_data/'):\n",
    "    os.makedirs('../data/cleaned_data/')\n",
    "\n",
    "# save merged dataframe as csv\n",
    "df_raw.to_csv('../data/cleaned_data/cleaned_data.csv')\n",
    "df_cleaned = df_raw.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Bereiten Sie, soweit für Ihre Modellierung nötig, die Variablen geeignet auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names\n",
    "column_names = df_cleaned.columns\n",
    "\n",
    "# Get categorical columns\n",
    "categorical_columns = df_cleaned.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Get numerical columns\n",
    "#numerical_columns = df_cleaned.select_dtypes(include=\"number\").columns\n",
    "\n",
    "# Create pipeline for categorical columns\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create transformer for all columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical_pipeline', categorical_pipeline, categorical_columns)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Fit and transform data\n",
    "df_cleaned = preprocessor.fit_transform(df_cleaned)\n",
    "\n",
    "# Convert to dataframe\n",
    "df_cleaned = pd.DataFrame(df_cleaned, columns=['lag_1', \n",
    "                                               'lag_2', \n",
    "                                               'lag_3', \n",
    "                                               'lag_4', \n",
    "                                               'lag_5', \n",
    "                                               'lag_6', \n",
    "                                               'lag_7', \n",
    "                                               'Fahrerlaubnis', \n",
    "                                               'Regional_Code', \n",
    "                                               'Vorversicherung', \n",
    "                                               'Jahresbeitrag', \n",
    "                                               'Vertriebskanal', \n",
    "                                               'Kundentreue', \n",
    "                                               'id', \n",
    "                                               'Interesse', \n",
    "                                               'Alter'])\n",
    "\n",
    "# Put id column as first column\n",
    "# and Interesse as last column\n",
    "df_cleaned = df_cleaned[['id', \n",
    "                         'lag_1', \n",
    "                         'lag_2', \n",
    "                         'lag_3', \n",
    "                         'lag_4', \n",
    "                         'lag_5', \n",
    "                         'lag_6', \n",
    "                         'lag_7', \n",
    "                         'Fahrerlaubnis', \n",
    "                         'Regional_Code', \n",
    "                         'Vorversicherung', \n",
    "                         'Jahresbeitrag', \n",
    "                         'Vertriebskanal', \n",
    "                         'Kundentreue', \n",
    "                         'Alter', \n",
    "                         'Interesse']]                                             \n",
    "\n",
    "# change unnecessary floats to int\n",
    "float_columns = ['Fahrerlaubnis', \n",
    "                 'Regional_Code', \n",
    "                 'Vorversicherung', \n",
    "                 'Vertriebskanal', \n",
    "                 'Kundentreue', \n",
    "                 'Alter', \n",
    "                 'Interesse']\n",
    "df_cleaned[float_columns] = df_cleaned[float_columns].astype('int64')\n",
    "\n",
    "# Check wether data path exists\n",
    "if not os.path.exists('../data/encoded_data/'):\n",
    "    os.makedirs('../data/encoded_data/')\n",
    "\n",
    "# save merged dataframe as csv\n",
    "df_cleaned.to_csv('../data/encoded_data/encoded_data.csv')\n",
    "df_encoded = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>lag_6</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>Fahrerlaubnis</th>\n",
       "      <th>Regional_Code</th>\n",
       "      <th>Vorversicherung</th>\n",
       "      <th>Jahresbeitrag</th>\n",
       "      <th>Vertriebskanal</th>\n",
       "      <th>Kundentreue</th>\n",
       "      <th>Alter</th>\n",
       "      <th>Interesse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>317635.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>124</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>337993.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>125</td>\n",
       "      <td>213</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160325.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>27204.0</td>\n",
       "      <td>124</td>\n",
       "      <td>114</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141620.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>31999.0</td>\n",
       "      <td>152</td>\n",
       "      <td>251</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75060.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>28262.0</td>\n",
       "      <td>26</td>\n",
       "      <td>60</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  lag_1  lag_2  lag_3  lag_4  lag_5  lag_6  lag_7  Fahrerlaubnis  \\\n",
       "0  317635.0    1.0    0.0    0.0    1.0    0.0    0.0    1.0              1   \n",
       "1  337993.0    1.0    0.0    0.0    0.0    1.0    0.0    1.0              1   \n",
       "2  160325.0    1.0    0.0    0.0    0.0    1.0    0.0    1.0              1   \n",
       "3  141620.0    0.0    1.0    0.0    1.0    0.0    0.0    1.0              1   \n",
       "4   75060.0    1.0    0.0    0.0    0.0    1.0    0.0    1.0              1   \n",
       "\n",
       "   Regional_Code  Vorversicherung  Jahresbeitrag  Vertriebskanal  Kundentreue  \\\n",
       "0             15                1         2630.0             124           74   \n",
       "1             28                0         2630.0             125          213   \n",
       "2             33                0        27204.0             124          114   \n",
       "3             46                1        31999.0             152          251   \n",
       "4             49                0        28262.0              26           60   \n",
       "\n",
       "   Alter  Interesse  \n",
       "0     76          0  \n",
       "1     43          0  \n",
       "2     20          0  \n",
       "3     24          0  \n",
       "4     51          0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>Regional_Code</th>\n",
       "      <th>Jahresbeitrag</th>\n",
       "      <th>Kundentreue</th>\n",
       "      <th>Alter</th>\n",
       "      <th>Interesse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>317635.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>337993.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>213</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160325.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>27204.0</td>\n",
       "      <td>114</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141620.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46</td>\n",
       "      <td>31999.0</td>\n",
       "      <td>251</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75060.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49</td>\n",
       "      <td>28262.0</td>\n",
       "      <td>60</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  lag_4  lag_5  Regional_Code  Jahresbeitrag  Kundentreue  Alter  \\\n",
       "0  317635.0    1.0    0.0             15         2630.0           74     76   \n",
       "1  337993.0    0.0    1.0             28         2630.0          213     43   \n",
       "2  160325.0    0.0    1.0             33        27204.0          114     20   \n",
       "3  141620.0    1.0    0.0             46        31999.0          251     24   \n",
       "4   75060.0    0.0    1.0             49        28262.0           60     51   \n",
       "\n",
       "   Interesse  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop features from df_encode with feature importance below 0.05\n",
    "# Also drop Features Vertriebskanal and Vorversicherung, since they highly correlate with more important features\n",
    "df_encoded = df_encoded.drop(['lag_1',\n",
    "                              'lag_2',\n",
    "                              'lag_3',\n",
    "                              'lag_6',\n",
    "                              'lag_7',\n",
    "                              'Fahrerlaubnis',\n",
    "                              'Vertriebskanal',\n",
    "                              'Vorversicherung',\n",
    "                              'id'], axis=1)\n",
    "df_encoded.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellvergleich\n",
    "\n",
    "Entscheiden Sie sich für ein geeignetes Modell zur Prognose der Kundenaffinität. Erläutern Sie wie Sie dabei vorgehen und begründen Sie Ihre Entscheidung."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Logistische Regression:__\n",
    "\n",
    "Die logistische Regression ist eine weit verbreitete Methode zur Vorhersage von binären Ergebnissen. Sie modelliert die Wahrscheinlichkeit, dass eine Beobachtung einer bestimmten Klasse angehört, basierend auf einer Kombination von Eingangsvariablen. Das Modell kann dann die Wahrscheinlichkeit schätzen, dass ein Kunde affin oder nicht affin ist und eine Entscheidungsgrenze festlegen, um die Vorhersage zu treffen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Random Forest Classifier:__\n",
    "\n",
    "Ein Random Forest Classifier ist ein Ensemble-Modell, das aus mehreren Entscheidungsbäumen besteht. Jeder Baum wird auf einem zufälligen Teil des Datensatzes trainiert, und die Vorhersage erfolgt durch Abstimmung der Vorhersagen der einzelnen Bäume. Random Forests sind in der Regel robust gegenüber Overfitting und können gut mit einer Mischung aus kategorischen und numerischen Variablen umgehen. Sie können auch die wichtigsten Merkmale identifizieren, die zur Vorhersage beitragen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Boosting-Modelle:__\n",
    "\n",
    "Gradient Boosting-Modelle wie der Gradient Boosting Classifier oder der XGBoost Classifier sind ensemblebasierte Modelle, die durch die Kombination mehrerer schwacher Lernalgorithmen starke Vorhersagemodelle erstellen. Sie sind bekannt für ihre hohe Vorhersagegenauigkeit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Neuronale Netzwerke:__\n",
    "\n",
    "Neuronale Netzwerke sind leistungsstarke Modelle, die in der Lage sind, komplexe Zusammenhänge in den Daten zu erfassen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hauptvorteile von RF::\n",
    "\n",
    "- Random Forests sind in der Regel robuster gegenüber Overfitting als logistische Regressionen, da sie mehrere Entscheidungsbäume verwenden, um eine Vorhersage zu treffen, anstatt nur einen einzigen Entscheidungsbaum zu verwenden.\n",
    "- Random Forests können gut mit einer Mischung aus kategorischen und numerischen Variablen umgehen, während logistische Regressionen nur numerische Variablen verarbeiten können.\n",
    "- Random Forests können die wichtigsten Merkmale identifizieren, die zur Vorhersage beitragen, während logistische Regressionen nur die Koeffizienten der einzelnen Merkmale liefern können.\n",
    "- Robustheit gegenüber Ausreißern: Aggregationen mehrerer Entscheidungsbäume reduziert den Einfluss von Ausreißern.\n",
    "- Nichtlinearität: Random Forests sind nicht parametrisch und können daher nichtlineare Zusammenhänge zwischen Merkmalen und Zielvariablen modellieren."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellbuilding\n",
    "\n",
    "1. Trainieren Sie das von Ihnen gewählte Modell. Wählen Sie geeignete Metriken um die Güte des finalen Modells zu beurteilen.\n",
    "2. Zeigen Sie, welche Variablen und Zusammenhänge für Ihre finales Modell relevant sind.\n",
    "3. Überlegen Sie sich (ohne Umsetzung) wie Sie Ihr Modell weiter optimieren können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 17:27:39,993 Train Accuracy: 0.9999978639690019\n",
      "2023-05-21 17:27:40,094 Train Precision: 1.0\n",
      "2023-05-21 17:27:40,195 Train Recall: 0.9999957248697154\n",
      "2023-05-21 17:27:40,296 Train F1: 0.9999978624302884\n"
     ]
    }
   ],
   "source": [
    "# Train a random forest classifier model \n",
    "# and select suitable hyperparameters\n",
    "# and metrics to view the models performance\n",
    "\n",
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_encoded.drop(['Interesse'], axis=1),\n",
    "                                                    df_encoded['Interesse'],\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Use random oversampler to balance dataset on Interesse = 1\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns=['Interesse'])\n",
    "\n",
    "# Predict on train set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Evaluate model on train set\n",
    "logger.info(f'Train Accuracy: {accuracy_score(y_train, y_train_pred)}')\n",
    "logger.info(f'Train Precision: {precision_score(y_train, y_train_pred)}')\n",
    "logger.info(f'Train Recall: {recall_score(y_train, y_train_pred)}')\n",
    "logger.info(f'Train F1: {f1_score(y_train, y_train_pred)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the models performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 17:27:44,541 Test Accuracy: 0.9450309011164274\n",
      "2023-05-21 17:27:44,586 Test Precision: 0.9040568739216448\n",
      "2023-05-21 17:27:44,631 Test Recall: 0.9959398137090996\n",
      "2023-05-21 17:27:44,676 Test F1: 0.9477766360937359\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "logger.info(f'Test Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "logger.info(f'Test Precision: {precision_score(y_test, y_pred)}')\n",
    "logger.info(f'Test Recall: {recall_score(y_test, y_pred)}')\n",
    "logger.info(f'Test F1: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether data path exists\n",
    "if not os.path.exists('../models/'):\n",
    "    os.makedirs('../models/')\n",
    "\n",
    "# Save model with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "with open(f'../models/cv_model_{timestamp}.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train another model using k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores:\n",
      "Accuracy: 0.9568778605484705\n",
      "Precision: 0.9209845098017443\n",
      "Recall: 0.9995095693779904\n",
      "F1-Score: 0.9586411129263478\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model using k-fold cross validation\n",
    "# and select suitable hyperparameters\n",
    "# and metrics to view the models performance\n",
    "\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "# Create the Random Forest Classifier model\n",
    "cv_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "k = 5\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_validate(model, X_resampled, y_resampled, cv=k, scoring=scoring)\n",
    "\n",
    "# Fit the model on the entire dataset\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Calculate the average scores across all folds\n",
    "mean_accuracy = scores['test_accuracy'].mean()\n",
    "mean_precision = scores['test_precision'].mean()\n",
    "mean_recall = scores['test_recall'].mean()\n",
    "mean_f1 = scores['test_f1'].mean()\n",
    "\n",
    "# Print the average scores\n",
    "print(\"Average Scores:\")\n",
    "print(\"Accuracy:\", mean_accuracy)\n",
    "print(\"Precision:\", mean_precision)\n",
    "print(\"Recall:\", mean_recall)\n",
    "print(\"F1-Score:\", mean_f1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Score:\n",
    "Accuracy_score berechnet den Prozentsatz der vom Modell getroffenen korrekten Vorhersagen aus allen Vorhersagen. Sie wird berechnet, indem die Anzahl der richtigen Vorhersagen durch die Gesamtzahl der Vorhersagen dividiert wird. Eine hohe Genauigkeitsbewertung weist auf eine insgesamt hohe Vorhersageleistung des Modells hin. Allerdings reicht die Genauigkeit allein möglicherweise nicht aus, wenn der Datensatz unausgeglichen ist.\n",
    "\n",
    "Precision Score:\n",
    "Precision_score misst den Anteil korrekt vorhergesagter positiver Instanzen an allen als positiv vorhergesagten Instanzen. Sie wird berechnet, indem die Anzahl der wahr-positiven Ergebnisse durch die Summe der wahr-positiven und falsch-positiven Ergebnisse dividiert wird. Präzision konzentriert sich auf die Qualität positiver Vorhersagen. Eine hohe Präzisionsbewertung weist auf eine niedrige Falsch-Positiv-Rate hin, was bedeutet, dass das Modell, wenn es eine positive Klasse vorhersagt, wahrscheinlich richtig ist.\n",
    "\n",
    "Recall Score:\n",
    "Recall_score, auch bekannt als Sensitivität oder True-Positive-Rate, misst den Anteil korrekt vorhergesagter positiver Instanzen an allen tatsächlich positiven Instanzen. Sie wird berechnet, indem die Anzahl der wahr-positiven Ergebnisse durch die Summe der wahr-positiven und falsch-negativen Ergebnisse dividiert wird. Recall konzentriert sich auf die Fähigkeit des Modells, alle positiven Instanzen zu finden, ohne welche zu übersehen. Ein hoher Erinnerungswert weist auf eine niedrige Falsch-Negativ-Rate hin, was bedeutet, dass das Modell einen großen Anteil positiver Fälle korrekt identifizieren kann.\n",
    "\n",
    "F1 Score:\n",
    "Der F1_score ist das harmonische Mittel aus Präzision und Erinnerung. Es bietet ein Gleichgewicht zwischen Präzision und Erinnerung und ist nützlich, wenn Sie sowohl falsch-positive als auch falsch-negative Ergebnisse berücksichtigen möchten. Sie wird wie folgt berechnet: 2 * ((Präzision * Rückruf) / (Präzision + Rückruf)). Der F1-Score ist eine einzelne Metrik, die Präzision und Erinnerung kombiniert. Ein hoher F1-Score weist auf eine gute Gesamtleistung hin, wenn man sowohl Präzision als auch Rückruf berücksichtigt.\n",
    "\n",
    "Zusammenfassend lässt sich sagen, dass Accuracy_score ein Gesamtleistungsmaß bietet, während Precision_score, Recall_score und f1_score Einblicke in verschiedene Aspekte der Modellleistung bieten. Berücksichtigen Sie Ihre spezifischen Anforderungen und die Art Ihres Problems, um zu bestimmen, welche Metrik(en) für Ihre Bewertung am wichtigsten sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether data path exists\n",
    "if not os.path.exists('../models/'):\n",
    "    os.makedirs('../models/')\n",
    "\n",
    "# Save model with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "with open(f'../models/cv_model_{timestamp}.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible strategies to optimize the Random Forrest Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Feature Selection:__\n",
    "\n",
    "Bewerten Sie die Bedeutung jedes Features in Ihrem Modell und erwägen Sie, weniger informative oder stark korrelierte Features zu entfernen. Dies kann durch die Untersuchung der Feature-Wichtigkeiten erreicht werden, die das Random-Forest-Modell liefert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.182870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_4</th>\n",
       "      <td>0.169762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kundentreue</th>\n",
       "      <td>0.158571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_5</th>\n",
       "      <td>0.152979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jahresbeitrag</th>\n",
       "      <td>0.142746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alter</th>\n",
       "      <td>0.130988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regional_Code</th>\n",
       "      <td>0.062085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               importance\n",
       "id               0.182870\n",
       "lag_4            0.169762\n",
       "Kundentreue      0.158571\n",
       "lag_5            0.152979\n",
       "Jahresbeitrag    0.142746\n",
       "Alter            0.130988\n",
       "Regional_Code    0.062085"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the most important features\n",
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Recursive Feature Elimination (RFE):__\n",
    "\n",
    "Rekursive Feature-Eliminierung (RFE) ist eine Feature-Auswahlmethode, die ein Modell anpasst und das schwächste Feature (oder die schwächsten Features) entfernt, bis die angegebene Anzahl von Features erreicht ist. Es basiert auf der Idee, dass gute Vorhersagemodelle nicht unbedingt alle Funktionen erfordern. RFE kann verwendet werden, um die Modellleistung zu verbessern und Überanpassungen zu reduzieren, indem eine Teilmenge von Merkmalen ausgewählt wird, die für die Zielvariable am relevantesten sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Hyperparameter Tuning:__ \n",
    "\n",
    "Zufällige Wälder verfügen über verschiedene Hyperparameter, die angepasst werden können, um die Leistung zu verbessern und Überanpassungen zu reduzieren. Zu den wichtigsten Hyperparametern gehören die Anzahl der Bäume (n_estimators), die maximale Tiefe der Bäume (max_ Depth) und die minimale Anzahl von Stichproben, die zum Teilen eines internen Knotens erforderlich sind (min_samples_split). Mithilfe von Techniken wie der Rastersuche oder der Zufallssuche können Sie optimale Hyperparameterwerte finden, die Modellkomplexität und Leistung in Einklang bringen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "\n",
    "Eine Analyse der Korrelation zwischen den Merkmalen und der Zielgröße kann helfen, redundante oder stark korrelierte Merkmale zu identifizieren. In solchen Fällen kann eines der korrelierten Merkmale entfernt werden, um eine Überanpassung zu vermeiden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Cross-Validation:__ \n",
    "\n",
    "Anstatt das Modell nur anhand des Trainingssatzes zu bewerten, wird eine Kreuzvalidierung durchgeführt, um seine Leistung bei mehreren train-test-splits zu bewerten. Dies trägt zu einer zuverlässigeren Schätzung der Leistung des Modells bei und kann Aufschluss darüber geben, ob eine Überanpassung vorliegt.\n",
    "\n",
    "Der Vorteil der Kreuzvalidierung besteht darin, dass sie im Vergleich zu einem einzelnen train-test-split eine robustere Schätzung der Modellleistung liefert. Es hilft bei der Beurteilung der Fähigkeit des Modells, auf unsichtbare Daten zu verallgemeinern, und verringert den Einfluss der spezifischen Datenaufteilung auf die Bewertung.\n",
    "Um eine Kreuzvalidierung in scikit-learn durchzuführen, können Sie die Funktion „cross_val_score“ oder die Funktion „cross_validate“ verwenden und dabei die Anzahl der Faltungen (cv-Parameter) und die gewünschte Bewertungsmetrik angeben. Diese Funktionen übernehmen die Datenaufteilung und Modellauswertung automatisch und erleichtern so die Durchführung einer Kreuzvalidierung mit zufälligen Gesamtstrukturen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Increase Training Data:__ \n",
    "\n",
    "Erhalten Sie nach Möglichkeit mehr Trainingsdaten, um eine umfassendere Darstellung der zugrunde liegenden Muster zu erhalten. Ein größerer Datensatz kann dazu beitragen, das Modell besser zu verallgemeinern und eine Überanpassung zu reduzieren."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addendum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5625 candidates, totalling 16875 fits\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=2, min_samples_leaf=3, min_samples_split=8, n_estimators=100; total time=  18.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=2, min_samples_leaf=3, min_samples_split=8, n_estimators=100; total time=  18.8s\n",
      "[CV] END bootstrap=True, max_depth=10, max_features=2, min_samples_leaf=3, min_samples_split=8, n_estimators=100; total time=  19.1s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 102\u001b[0m\n\u001b[1;32m     98\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator \u001b[39m=\u001b[39m rf, param_grid \u001b[39m=\u001b[39m param_grid,\n\u001b[1;32m     99\u001b[0m                             cv \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, n_jobs \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[39m# Fit the grid search to the data\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m    104\u001b[0m \u001b[39m# Get best parameters\u001b[39;00m\n\u001b[1;32m    105\u001b[0m grid_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1389\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1388\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huk_coding_challenge/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train a random forest classifier model\n",
    "# select suitable hyperparamters with grit search\n",
    "# select metrics to view the models performance\n",
    "\n",
    "# Get categorical columns\n",
    "categorical_columns = df_cleaned.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Get numerical columns\n",
    "numerical_columns = df_cleaned.select_dtypes(include=\"number\").columns\n",
    "\n",
    "# Create pipeline for categorical columns\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create transformer for all columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical_pipeline', categorical_pipeline, categorical_columns)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Fit and transform data\n",
    "df_cleaned = preprocessor.fit_transform(df_cleaned)\n",
    "\n",
    "# Convert to dataframe\n",
    "df_cleaned = pd.DataFrame(df_cleaned, columns=['lag_1', \n",
    "                                               'lag_2', \n",
    "                                               'lag_3', \n",
    "                                               'lag_4', \n",
    "                                               'lag_5', \n",
    "                                               'lag_6', \n",
    "                                               'lag_7', \n",
    "                                               'Fahrerlaubnis', \n",
    "                                               'Regional_Code', \n",
    "                                               'Vorversicherung', \n",
    "                                               'Jahresbeitrag', \n",
    "                                               'Vertriebskanal', \n",
    "                                               'Kundentreue', \n",
    "                                               'id', \n",
    "                                               'Interesse', \n",
    "                                               'Alter'])\n",
    "\n",
    "# Put id column as first column\n",
    "# and Interesse as last column\n",
    "df_cleaned = df_cleaned[['id', \n",
    "                         'lag_1', \n",
    "                         'lag_2', \n",
    "                         'lag_3', \n",
    "                         'lag_4', \n",
    "                         'lag_5', \n",
    "                         'lag_6', \n",
    "                         'lag_7', \n",
    "                         'Fahrerlaubnis', \n",
    "                         'Regional_Code', \n",
    "                         'Vorversicherung', \n",
    "                         'Jahresbeitrag', \n",
    "                         'Vertriebskanal', \n",
    "                         'Kundentreue', \n",
    "                         'Alter', \n",
    "                         'Interesse']]\n",
    "\n",
    "# change unnecessary floats to int\n",
    "float_columns = ['Fahrerlaubnis', \n",
    "                 'Regional_Code', \n",
    "                 'Vorversicherung', \n",
    "                 'Vertriebskanal', \n",
    "                 'Kundentreue', \n",
    "                 'Alter']\n",
    "df_cleaned[float_columns] = df_cleaned[float_columns].astype('int64')\n",
    "\n",
    "df_encoded = df_cleaned.copy()\n",
    "\n",
    "# Train a random forest classifier model\n",
    "# select suitable hyperparamters with grit search\n",
    "# select metrics to view the models performance\n",
    "\n",
    "# train, test, split\n",
    "# Split data into train and test\n",
    "X = df_encoded.drop(['Interesse'], axis=1)\n",
    "y = df_encoded['Interesse']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_samples_leaf': [3, 4, 5, 6, 7],\n",
    "    'min_samples_split': [8, 10, 12, 14, 16],\n",
    "    'n_estimators': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,\n",
    "                            cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters\n",
    "grid_search.best_params_\n",
    "\n",
    "# Get best estimator\n",
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "# Get metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=2)\n",
    "recall = recall_score(y_test, y_pred, pos_label=2)\n",
    "f1 = f1_score(y_test, y_pred, pos_label=2)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1: {f1}')\n",
    "\n",
    "# Check wether data path exists\n",
    "if not os.path.exists('../models/'):\n",
    "    os.makedirs('../models/')\n",
    "\n",
    "# save model\n",
    "pickle.dump(best_grid, open('../models/model.pkl', 'wb'))\n",
    "\n",
    "# save preprocessor\n",
    "pickle.dump(preprocessor, open('../models/preprocessor.pkl', 'wb'))\n",
    "\n",
    "# save metrics\n",
    "metrics = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "pickle.dump(metrics, open('../models/metrics.pkl', 'wb'))\n",
    "\n",
    "# save predictions\n",
    "predictions = {'y_pred': y_pred}\n",
    "pickle.dump(predictions, open('../models/predictions.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huk_coding_challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "913db32498f814a7e956b858dc9ab26e0e62ace447c646841c6d05c67a28a2e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
